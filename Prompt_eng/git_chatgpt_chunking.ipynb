{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sfY6yEoj9NDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tiktoken openai tenacity --upgrade"
      ],
      "metadata": {
        "id": "G0449s_j1G96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01941663-43a5-4d64-d41c-31804d46dd65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Installing collected packages: h11, tiktoken, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7BPKSbi02mW"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OpenAI.api_key = \"Enter API Key\"\n",
        "client = OpenAI()\n",
        "\n",
        "# Get the openai secret key:\n"
      ],
      "metadata": {
        "id": "yZIdScda1KAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "071c8f7f-51b2-4545-c9bf-2ef5845d7920"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'OpenAI' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-113167156c93>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mOpenAI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Enter API Key\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get the openai secret key:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'OpenAI' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_headings = [\n",
        "    \"I. Introduction A. Definition of the 2008 Financial Crisis B. Overview of the Causes and Effects of the Crisis C. Importance of Understanding the Crisis\",\n",
        "    \"II. Historical Background A. Brief History of the US Financial System B. The Creation of the Housing Bubble C. The Growth of the Subprime Mortgage Market\",\n",
        "    \"III. Key Players in the Crisis A. Government Entities B. Financial Institutions C. Homeowners and Borrowers\",\n",
        "    \"IV. Causes of the Crisis A. The Housing Bubble and Subprime Mortgages B. The Role of Investment Banks and Rating Agencies C. The Failure of Regulatory Agencies D. Deregulation of the Financial Industry\",\n",
        "    \"V. The Domino Effect A. The Spread of the Crisis to the Global Financial System B. The Impact on the Real Economy C. The Economic Recession\",\n",
        "    \"VI. Government Responses A. The Troubled Asset Relief Program (TARP) B. The American Recovery and Reinvestment Act C. The Dodd-Frank Wall Street Reform and Consumer Protection Act\",\n",
        "    \"VII. Effects on Financial Institutions A. Bank Failures and Bailouts B. Stock Market Decline C. Credit Freeze\",\n",
        "    \"VIII. Effects on Homeowners and Borrowers A. Foreclosures and Bankruptcies B. The Loss of Home Equity C. The Impact on Credit Scores\",\n",
        "    \"IX. Effects on the Global Economy A. The Global Financial Crisis B. The Impact on Developing Countries C. The Role of International Organizations\",\n",
        "    \"X. Criticisms and Controversies A. Bailouts for Financial Institutions B. Government Intervention in the Economy C. The Role of Wall Street in the Crisis\",\n",
        "    \"XI. Lessons Learned A. The Need for Stronger Regulation B. The Importance of Transparency C. The Need for Better Risk Management\",\n",
        "    \"XII. Reforms and Changes A. The Dodd-Frank Wall Street Reform and Consumer Protection Act B. Changes in Regulatory Agencies C. Changes in the Financial Industry\",\n",
        "    \"XIII. Current Economic Situation A. Recovery from the Crisis B. Impact on the Job Market C. The Future of the US Economy\",\n",
        "    \"XIV. Comparison to Previous Financial Crises A. The Great Depression B. The Savings and Loan Crisis C. The Dot-Com Bubble\",\n",
        "    \"XV. Economic and Social Impacts A. The Widening Wealth Gap B. The Rise of Populist Movements C. The Long-Term Effects on the Economy\",\n",
        "    \"XVI. The Role of Technology A. The Use of Technology in the Financial Industry B. The Impact of Technology on the Crisis C. The Future of the Financial Industry\",\n",
        "    \"XVII. Conclusion A. Recap of the Causes and Effects of the Crisis B. The Importance of Learning from the Crisis C. Final Thoughts\",\n",
        "    \"XVIII. References A. List of Sources B. Additional Reading C. Further Research\",\n",
        "    \"XIX. Glossary A. Key Terms B. Definitions\",\n",
        "    \"XX. Appendix A. Timeline of the Crisis B. Financial Statements of Key Players C. Statistical Data on the Crisis\",\n",
        "]"
      ],
      "metadata": {
        "id": "oG1xHScL1J9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def chatgpt_completion_with_backoff(**kwargs):\n",
        "    return client.chat.completions.create(**kwargs)"
      ],
      "metadata": {
        "id": "b2ixKYkl1J6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_for_chat_history(response: client = False, user_prompt=None):\n",
        "    if user_prompt:\n",
        "        return {'role': 'user', 'content': user_prompt}\n",
        "    return {'role': 'assistant', 'content': response.choices[0].message.content}"
      ],
      "metadata": {
        "id": "KKqld4Jc1c1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0613\",\n",
        "        \"gpt-3.5-turbo-16k-0613\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "    }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = (\n",
        "            4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        )\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\n",
        "            \"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\"\n",
        "        )\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\n",
        "            \"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\"\n",
        "        )\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens"
      ],
      "metadata": {
        "id": "W6ZNECaV1cyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
        "  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "  {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "]\n",
        "\n",
        "model = \"gpt-3.5-turbo-0301\"\n",
        "\n",
        "print(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n",
        "# Should show ~126 total_tokens"
      ],
      "metadata": {
        "id": "dob_W7ki1J3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a helpful assistant for a financial news website. You are writing a series of articles about the 2008 financial crisis. You have been given a list of headings for each article. You need to write a short paragraph for each heading. You can use the headings as a starting point for your writing.\\n\\n\"\n",
        "system_prompt += \"All of the subheadings:\\n\"\n",
        "\n",
        "# Add all of the subheadings to the system prompt to give the model context:\n",
        "for heading in article_headings:\n",
        "    system_prompt += f\"{heading}\\n\"\n",
        "\n",
        "# Create the chat history object:\n",
        "chat_history =  []\n",
        "\n",
        "# Add in the system message:\n",
        "chat_history.append({'role': 'system', 'content': system_prompt})"
      ],
      "metadata": {
        "id": "tLlzRR9k1J0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing messages based on a token size"
      ],
      "metadata": {
        "id": "s3qZ_ZKn1uRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This will ensure that if the token count goes over the limit, the last message will be removed,\n",
        "# to ensure that the token count is reduced as the chat history grows:\n",
        "MAX_TOKEN_SIZE = 2048"
      ],
      "metadata": {
        "id": "3Vn5F3Fg1qbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over all of the headings and generate a chunk for each one\n",
        "for heading in article_headings:\n",
        "    # Add on a user prompt to the chat history object:\n",
        "    chat_history.append(\n",
        "        {\"role\": \"user\", \"content\": f\"Write a short paragraph about {heading}\"}\n",
        "    )\n",
        "\n",
        "    # Tell ChatGPT to generate a response:\n",
        "    response = chatgpt_completion_with_backoff(\n",
        "        model=\"gpt-3.5-turbo\", messages=chat_history\n",
        "    )\n",
        "\n",
        "    # Get the response from the ChatGPT object:\n",
        "    chat_history.append(parse_for_chat_history(response))\n",
        "\n",
        "    # Whilst the Chat history object is more than 2048 tokens, remove the oldest non-system message:\n",
        "    while num_tokens_from_messages(chat_history, model) > 2048:\n",
        "        # Find the index of the first non-system message\n",
        "        non_system_msg_index = next(\n",
        "            (i for i, msg in enumerate(chat_history) if msg[\"role\"] != \"system\"), None\n",
        "        )\n",
        "\n",
        "        # If there is a non-system message, remove it\n",
        "        if non_system_msg_index is not None:\n",
        "            chat_history.pop(non_system_msg_index)\n",
        "\n",
        "    # Print the response:\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "z8oTIw531qWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oAFCHz7q1qS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "haX8DZLL1qQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5Ua-q-q1JtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}